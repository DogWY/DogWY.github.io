---
title: 线性模型
cover: ./images/theme/defaultCover.png
date: 2023-09-30 12:08:44
tags: 机器学习
categories: 学习笔记
Typora-root-url: 线性模型
---

# 线性模型

## 基本形式

给定由$d$个属性描述的示例$x=(x_1;x_2;...;x_d)$,其中$x_i$是$x$在第$i$个属性上的取值, 线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数, 即:

$$
f(x) = w_1 x_1 + w_2 x_2 + ... + w_d x_d +b
$$

或者写成向量形式:

$$
f(x) = \boldsymbol{w^T x} + b
$$

线性模型形式简单, 易于建模, 并且蕴含着机器学习中的一些重要的基本思想. 此外, 许多功能强大的非线性模型(nonlinear model)可以在线性模型的基础上通过引入层级结构或高纬映射而得.

此外, 由于线性模型形式上十分直观, 因此线性模型有很好的可解释性.

## 线性回归

线性回归: 在给定数据集上试图学得一个线性模型以尽可能准确地预测实际输出标记.

核心即为通过学习确定$\boldsymbol{w}$和$\boldsymbol{b}$

### 线性回归中的属性

连续属性可以直接使用, 但是对于离散属性就有些特殊:

- 如果离散属性之间存在"序"关系, 则可以通过连续化将其转化为连续值. 比如对于"高度"的取值"高","中","低"可以转化为$\{1.0, 0.5, 0.0\}$
- 如果离散属性之间不存在"序"关系, 则需根据其取值范围, 将其转化为多维向量(加入有$k$中可能取值, 则转化为$k$维向量). 比如对于"瓜"的可能取值"西瓜", "南瓜", "黄瓜"可转化为$\{(0,0,1),(0,1,0),(1,0,0)\}$

> 如果将无"序"属性连续化, 则会不恰当的引入"序"关系, 对后续距离计算等操作引入误差.

### 对于一元线性回归

此时线性回归试图学得:

$$
f(x_i) = w x_i + b, 使得f(x_i) \simeq y_i
$$

如何确定$w$和$b$呢? 核心思路为:

1. 衡量$f(x)$和$y$的差别;
2. 最小化$f(x)$和$y$的差别, 达到"近似";

> 显然, 只要能够数值化$f(x)$和$y$的差别, 我们就可以根据导数来最小化它

衡量差别有很多种方法, 其中最常用的是均方误差, 以下我们采用均方误差来进行推导, 即:

$$
\begin{equation}
\begin{split}
(w^*, b^*) & = \mathop{\arg\min}\limits_{(w, b)} \sum\limits_{i=1}^{m} (f(x_i) - y_i)^2 \\
& = \mathop{\arg\min}\limits_{(w, b)} \sum\limits_{i=1}^{m} (y_i - wx_i -b)^2
\end{split}
\end{equation}
$$

> 均方误差对应了常用的"欧氏距离", 基于均方误差最小化来进行模型求解的方法称为"最小二乘法"(least square method). 
>
> 在线性回归中, 最小二乘法就是试图找到一条直线, 使所有样本到直线上的欧氏距离最小.

求解$w$和$b$使$E_{(w,b)} = \sum\limits_{i=1}^{m} (y_i - wx_i -b)^2$最小化的过程, 称为线性回归模型的最小二乘"参数估计"(parameter estimation).

对于一元线性回归, 我们可以直接将$E_{(w,b)}$分别对$w$和$b$分别求导, 得到:

$$
\begin{equation}
\begin{split}
\frac{E_{(w,b)}}{\partial w} & = 2 (w\sum \limits_{i=1}^{m} x_i^2 - \sum \limits_{i=1}^{m}) (y_i - b)x_i \\
\frac{E_{(w,b)}}{\partial b} & = 2(mb - \sum \limits_{i=1}^{m} (y_i - wx_i))
\end{split}
\end{equation}
$$

然后令导数为零, 即可得到$w$和$b$的最优解:

$$
\begin{equation}
\begin{split}
w &= \frac{\sum\limits_{i=1}^{m} y_i(x_i - \overline{x})}{\sum\limits_{i=1}^{m} x_i^2 - \frac{1}{m} (\sum\limits_{i=1}^{m})^2} \\
b &= \frac{1}{m} \sum \limits_{i=1}^{m} (y_i - wx_i)
\end{split}
\end{equation}
$$

解得以上公式, 即解得一元线性回归模型.

### 多元线性回归

以上讨论了[一元线性回归模型](#对于一元线性回归), 而更一般的情况是样本会由$d$个属性描述, 此时线性回归试图学得:

$$
f(\boldsymbol{x}_i) = \boldsymbol{w}^T \boldsymbol{x}_i + b, 使得f(\boldsymbol{x}_i) \simeq y_i
$$

这称为"多元线性回归"(multivariate linear regression).

此时, 我们同样使用最小二乘法来对$w$和$b$进行估计. 为了方便讨论, 我们可以将$w$和$b$集中为向量形式:$\hat{w} = (\boldsymbol{w};b)$, 把数据集$D$表示为一个$m \times (d + 1)$大小的矩阵$X$如下所示:


$$
X = \begin{pmatrix}
\begin{split}
x_{11}&\ x_{12} \ ... \ x_{1d} \ 1\\
x_{21}&\ x_{22} \ ... \ x_{2d} \ 1\\
\vdots& \ \ \ \ \ \ddots \ \ \ \ \vdots\\
x_{m1}&\ x_{m2} \ ... \ x_{md} \ 1
\end{split}
\end{pmatrix} = 
\begin{pmatrix}
\begin{split}
&x_{1}^T \ 1\\
&x_{2}^T \ 1\\
&\ \vdots \ \ \ \ \vdots\\
&x_{m}^T \ 1
\end{split}
\end{pmatrix}
$$

此时如果我们再把标记也写成向量形式$\boldsymbol{y} = (y_1;y_2;...;y_m)$, 则我们可以得到新的目标函数:

$$
\hat{\boldsymbol{w}}^* = \mathop{\arg\min}\limits_{\hat{w}} (\boldsymbol{y} - \boldsymbol{X}\hat{\boldsymbol{w}})^T(\boldsymbol{y} - \boldsymbol{X}\hat{\boldsymbol{w}})
$$

令$E_{\hat{w}} = (\boldsymbol{y} - \boldsymbol{X}\hat{\boldsymbol{w}})^T(\boldsymbol{y} - \boldsymbol{X}\hat{\boldsymbol{w}})$, 则其对$\hat{\boldsymbol{w}}$求导可得:

$$
\frac{\partial E_{\hat{w}}}{\partial \hat{w}} = 2 \boldsymbol{X}^T (\boldsymbol{X} \hat{\boldsymbol{w}} - \boldsymbol{y})
$$

令上式为零可解得$\hat{\boldsymbol{w}}$最优解, 即解得一元线性回归模型.

#### 多元回归的问题

在最终求解$\hat{\boldsymbol{w}}$时会面临一个问题: $\boldsymbol{X}^T\boldsymbol{X}$往往不是满秩矩阵, 因此最终$\hat{\boldsymbol{w}}$可能有多个解. 

此时我们需要给予学习算法归纳偏好, 来选择其中一个解, 最常见的做法是引入`正则化项`.

> 此处涉及正则化内容, 暂不讨论

## 广义线性模型

在以上的线性模型中,我们希望模型的预测值能够尽可能地逼近标记$y$, 那么自然我们就会想到: 可否令模型预测值去逼近标签$y$的衍生物呢?

自然是可以的, 比如"对数线性回归"(log-linear regression):

$$
\ln y = \boldsymbol{w}^T \boldsymbol{x} + b
$$

它实际上实在试图让$e^{\boldsymbol{w}^T \boldsymbol{x} + b}$逼近标签$y$.

虽然看上去, "对数线性回归"仍是线性回归, 但是实质上它已经是在求输入空间到输出空间的非线性函数映射了.

由此, 更一般的, 我们可以考虑一个单调可微函数$g(\cdot)$, 令:

$$
y = g^{-1}(\boldsymbol{w}^T \boldsymbol{x} + b)
$$

这样我们就得到了"广义线性模型".

> $g(\cdot)$必须连续且光滑, 否则我们无法通过求导来计算最优解

## 参考资料

- 清华大学出版社 周志华 《机器学习》(西瓜书)
