# 标准化/Normalization

## LayerNorm

对单个数据样本的所有特征进行规范化。

> 一般在NLP任务中使用

$$
LN(x_i) = \gamma (\frac{x_i - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}}) + \beta
$$

其中，$x_i$是输入特征，$\mu_L$和$\sigma_L^2$是单个样本内所有特征的均值和方差，$\gamma$和$\beta$是可学习参数，$\epsilon$是防止除零的小值。

## BatchNorm

通过对每个特征在在小批量数据（batch）上计算均值和方差来进行规范化。

> 一般在图像任务中使用

公式：

$$
BN(x_i) = \gamma (\frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}) + \beta
$$

其中，$x_i$是输入特征，$\mu_B$是批量的均值，$\sigma+B^2$是批量的方差，$\gamma$和$\beta$是可学习参数，$\epsilon$是防止除零的小值。