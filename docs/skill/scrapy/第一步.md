# Scrapy快速上手

Scrapy是一个开源的Python爬虫框架。这里给出[官方文档](https://docs.scrapy.org/en/latest/)和一个[中文文档]()。

> 这里记录了本人在学习文档时的随记。

## 第一步

### 什么是Scrapy

Scrapy一个被设计用于**抓取和提取web站点的结构化数据**的应用框架，不过它**也可以用API提取数据**。

Scrapy的一个优势是**请求的调度是异步的**。

## 安装

```bash
pip install Scrapy
```

> 值得一提的是，scrapy支持pypy

## 教程

以一个[名人名言网站](https://quotes.toscrape.com/)为例，实操如何用Scrapy制作一个简单的爬虫。

在教程中，你将顺序进行如下操作：

1. 创建一个新的Scrapy项目
2. 写一个`spider`爬取一个站点并提取数据
3. 使用命令行工具导出爬取到的数据
4. 递归地爬取跟随链接🔗
5. 使用爬虫参数

### 构建一个项目

在你开始**爬**之前， 你需要先创建爬虫项目。

```bash
scrapy startproject tutorial
```

这将创建一个名为`tutorial`的项目，初始化后项目中有如下内容:

```
tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py
```

### 写一个爬虫

爬虫是一个由你定义并由Scrapy用于爬取网站的类，它必须是`scrapy.Spider`的一个子类。

爬虫要定义如何发送初始请求，如何跟踪网页中的链接，如何解析下载到的网页内容并从中提取数据。

现在在`tutorial/spiders`文件夹下创建文件`quotes_spider.py`并写入如下内容：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

As you can see～，我们的爬虫继承了`scrapy.Spider`并且定义一些属性和方法：

- `name`：Spider的唯一标识，在一个项目中它必须是唯一的。
- `start_requests()`：这个方法必须返回一个可迭代对象（你可以直接返回一个list，或者像以上代码一样，只做一个生成器），爬虫将会根据这个方法的返回值爬取数据。后续请求将依次从这些初始请求中生成。
- `parse()`：用来每个请求获取到的响应。这个函数的参数`response`是`TextResponse`的一个实例，用于保存页面内容，并具有处理页面内容的更多有用方法。

> `parse()`通常用来解析响应，提取数据，并且也会用来找到新的需要爬取的URL然后构建新的请求。

### 如何运行爬虫

在项目的根路径下：

```bash
scrapy crawl quotes
```

这个命令的使用了刚才定义的`name`为`quotes`的爬虫，然后scrapy将会发送一些请求到`quotes.toscrape.com`，正常情况下，你应该能看到跟一下内容相似的输出：

```bash
... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
```

现在你应该能在项目中找到两个新文件:`quotes-1.html`、`quotes-2.html`，其中保存到了爬虫刚刚请求到的网页内容。

### 刚刚发生了什么？

Scrapy调度爬虫的`start_requests`方法返回的`scrapy.Request`对象。在收到所有响应之后，Scrapy会实例化`Response`对象并调用与请求关联的回调方法（这里是`parse()`），并将响应作为参数传递。

### 一个`start_requests`的简写

`start_requests`并不必须是一个方法，他也可以是一个类属性。

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
```

### 提取数据

最佳地学习使用Scrapy提取数据的方式是通过Scrapy Shell使用选择器，现在运行：

```bash
scrapy shell 'http://quotes.toscrape.com/page/1/'
```

你将会看到：

```bash
[ ... Scrapy log here ... ]
2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>
[s]   item       {}
[s]   request    <GET http://quotes.toscrape.com/page/1/>
[s]   response   <200 http://quotes.toscrape.com/page/1/>
[s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>
[s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
>>>
```

通过这个shell，你能够在响应对象上使用CSS选择元素：

```bash
>>> response.css('title')
[<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]
```

`response.css('title')`的返回值是`SelectorList`对象，它是一个类数组，相当于一个包裹了XML/HTML的`Selector`的列表，`Selector`允许用户进一步地查询或提取数据。

为了从title元素中提取文本，你可以：

```bash
>>> response.css('title::text').getall()
['Quotes to Scrape']
```

这里有两个需要注意的点，一个是我们追加了`::text`来进行CSS查询，这意味着我们只想要获取直接隶属于`title`标签内部的本文元素。如果我们不指定`::text`，那我们面临如下结果：

```bash
>>> response.css('title').getall()
['<title>Quotes to Scrape</title>']
```

另一个是调用`.getall()`的结果是一个数组：一个selector可能就会有多个返回值，此时`.getall()`的结果是每个selector的多个返回值共同组成的一个数组（一维的，并起来了）。如果你只想要第一个结果，在这个案例中，你可以这么做：

```bash
>>> response.css('title::text').get()
'Quotes to Scrape'
```

当然，你也可以这样写：

```bash
>>> response.css('title::text')[0].get()
'Quotes to Scrape'
```

然而，直接在`SelectorList`上使用`.get()`方法可以别面出现`IndexError`或者`return None`的可能。

出了`getall()`和`get()`方法，你还可以使用`re()`方法来使用正则表达式查询：

```bash
>>> response.css('title::text').re(r'Quotes.*')
['Quotes to Scrape']
>>> response.css('title::text').re(r'Q\w+')
['Quotes']
>>> response.css('title::text').re(r'(\w+) to (\w+)')
['Quotes', 'Scrape']
```

为了方便找到正确的CSS选择器，你可以使用`view(response)`来使用Web浏览器查看响应内容，然后借助浏览器提供的开发者工具来辅助找到CSS选择器。

### XPath：一个简单的介绍

出了CSS选择器，Scrapy selector也支持使用XPath表达式：

```bash
>>> response.xpath('//title')
[<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]
>>> response.xpath('//title/text()').get()
'Quotes to Scrape'
```

XPath表达式十分强大。而且事实上，在Scrapy中使用的CSS选择器会被隐式地转换为XPath然后再进行查询。

虽然可能不像 CSS 选择器那样受欢迎，但 XPath 表达式提供了更多的功能，因为除了导航结构之外，它还可以查看内容。借助XPath，你可以进行类似这种查询：查询包含文本'Next Page'的超链接。这使得XPath特别适合去执行爬虫任务，并且即使你已经知道如何使用CSS选择器了，我们也仍然建议你使用XPath。本章只是Scrapy的简单介绍，我们会在之后的章节详细介绍Scrapy中的XPath。

### 提取引文和作者

现在你简单了解了如何查询和提取，现在让我们完善这个爬虫，让他能够爬取网页中的引文。

每个 http://quotes.toscrape.com 中的引文都是这样的格式：

```html
<div class="quote">
    <span class="text">“The world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.”</span>
    <span>
        by <small class="author">Albert Einstein</small>
        <a href="/author/Albert-Einstein">(about)</a>
    </span>
    <div class="tags">
        Tags:
        <a class="tag" href="/tag/change/page/1/">change</a>
        <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
        <a class="tag" href="/tag/thinking/page/1/">thinking</a>
        <a class="tag" href="/tag/world/page/1/">world</a>
    </div>
</div>
```

现在我们打开scrapy shell，然后摸索如何提取我们想要的数据：

```bash
scrapy shell 'http://quotes.toscrape.com'
```

然后我们获取所有的引文：

```bash
>>> response.css("div.quote")
```

以上查询返回的每一个Selector都允许我们更进一步的查询他们的子元素。让我们将第一个Selector分配给一个变量，然后我们就能直接在第一个Selector上应用CSS选择器。

```bash
>>> quote = response.css("div.quote")[0]
```

现在，让我们从`quote`中提取引文的`title`、`author`和`tags`：

```bash
>>> title = quote.css("span.text::text").get()
>>> title
'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'
>>> author = quote.css("small.author::text").get()
>>> author
'Albert Einstein'
```

获取的到tags是一个字符串列表，我们可以使用`.getall()`方法获取他们：

```bash
>>> tags = quote.css("div.tags a.tag::text").getall()
>>> tags
['change', 'deep-thoughts', 'thinking', 'world']
```

现在我们弄清楚了如何提取数据，现在我们就可以迭代地获取所有的引文元素：

```bash
>>> for quote in response.css("div.quote"):
...     text = quote.css("span.text::text").get()
...     author = quote.css("small.author::text").get()
...     tags = quote.css("div.tags a.tag::text").getall()
...     print(dict(text=text, author=author, tags=tags))
{'tags': ['change', 'deep-thoughts', 'thinking', 'world'], 'author': 'Albert Einstein', 'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'}
{'tags': ['abilities', 'choices'], 'author': 'J.K. Rowling', 'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”'}
    ... a few more of these, omitted for brevity
>>>
```

### 在我们的Spider中提取数据

现在让我们将以上逻辑实现到我们的Spider中。

通常一个Scrapy的Spider会生成许多字典，字典中存放着从页面中提取的数据。借助`yeild`，我们可以如下实现Spider：


```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }
```

运行以上代码，你应该能够看到如下输出：


```bash
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': "“I have not failed. I've just found 10,000 ways that won't work.”"}
```

### 存储爬取到的数据

存储爬取到的数据的最简单方法是使用Feed导出，就像如下命令：

```bash
scrapy crawl quotes -o quotes.json
```

以上代码会生成一个`quotes.json`文件保存爬取到的数据。

因为一些历史遗留问题，当你重复执行这个命令时，文件并不会被复写，而是会将新内容追加到原来的文件最后，变成类似这样的`[第一次结果][第二次结果]`形式。

你也能使用其他格式存储数据，比如Json Lines：

```bash
scrapy crawl quotes -o quotes.jl
```

Json Lines通常很有用，因为它是一个流式数据，他会更容易被扩展，而不会出现以上Json格式那样的追加问题。

在小项目中，以上的所有内容应该已经足够了。然而，如果你想执行更复杂的爬取，你可能会用到Scrapy的管道（Pipeline）。项目在初始化时，已经在`tutorial/pipelines.py`中预留好了项目管道的占位文件，如果您只想存储抓取到的项目，则不需要实现任何管道。

### 随后的链接

`随后的链接`的意思是，我们不仅要从Spider中声明的两个网页中爬取数据，还需要从网页中超链接指向的页面中爬取数据。

首先，我们回顾刚才爬取的页面，我们可以看到有一个指向下一页的超链接：

```html
<ul class="pager">
    <li class="next">
        <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>
    </li>
</ul>
```

我们首先获取到超链接元素：

```bash
>>> response.css('li.next a').get()
'<a href="/page/2/">Next <span aria-hidden="true">→</span></a>'
```

现在我们获取到的了元素，但是我们其实是想要获取这个标签的`href`属性对吧，此时Scrapy支持使用扩展CSS选择器：

```bash
>>> response.css('li.next a::attr(href)').get()
'/page/2/'
```

还有一种方法获取属性，就是hissing`attrib`属性

```bash
>>> response.css('li.next a').attrib['href']
'/page/2'
```

现在我们修改我们的爬虫，递归地跟随链接到下一页，并从中提取数据：

```bash
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
```

现在，我们的爬虫会在爬取之前的数据之后，再次通过`parse()`函数爬取下一页。

这里有一个需要注意的点，以上我们获取到的超链接其实是相对地址，我们需要通过`urljoin()`函数来构造绝对地址才能正常访问。

看过如上代码，我们能够看出一个Scrapy的机制，在`parse()`函数抛出一个Request对象时，Scrapy将会调度这个请求，并在请求结束时调用回调方法。

### 一个构建Request的简写

以上的逻辑可以被精简如下：

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
```

没错，`response.follow()`支持相对URL路径。

需要注意的是，`response.follow()`方法会返回一个Request对象，所以本质上还是`parse()`抛出了一个Request。

`response.follow()`方法还不止于此，他还支持进一步的简写，即传入一个selector：

```python
for href in response.css('li.next a::attr(href)'):
    yield response.follow(href, callback=self.parse)
```

再进一步，传入一个`<a>`标签的列表也可以：

```python
for a in response.css('li.next a'):
    yield response.follow(a, callback=self.parse)
```

> 需要注意的是，`response.follow()`不支持传入selector列表，即写法`response.follow(response.css('li.next a')[0])`非法的。

### 新的例子

这里再给出一个从网站中爬取作者信息的例子：

```python
import scrapy

class AuthorSpider(scrapy.Spider):
    name = 'author'

    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        # follow links to author pages
        for href in response.css('.author + a::attr(href)'):
            yield response.follow(href, self.parse_author)

        # follow pagination links
        for href in response.css('li.next a::attr(href)'):
            yield response.follow(href, self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).get(default='').strip()

        yield {
            'name': extract_with_css('h3.author-title::text'),
            'birthdate': extract_with_css('.author-born-date::text'),
            'bio': extract_with_css('.author-description::text'),
        }
```

仔细看看上面的代码，结合你对网页的理解，你可能会产生一个担忧，以上的代码会不会重复爬取作者的信息呢？

答案是不必担心，Scrapy默认会过滤掉URL重复的请求，避免对特定服务的大量重复请求。当然，这个也可以通过`DUPEFILTER_CLASS`来配置是否要过滤。

此外，还有一些常见的需求是使用多个页面的数据构建一个item，这可能要用到将数据附加给回调函数的技巧。

### 使用爬虫参数

我们可以通过可选的`-a`参数来，在启动爬虫时传入参数：

```bash
scrapy crawl quotes -o quotes-humor.json -a tag=humor
```

这些参数会被传递给爬虫的`__init__`方法，然后变成爬虫的一个属性。

然后我们就可以这样利用传入参数了：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        url = 'http://quotes.toscrape.com/'
        tag = getattr(self, 'tag', None)
        if tag is not None:
            url = url + 'tag/' + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

在传入参数为`tag=humor`时，你会注意到爬虫只爬取了`/humor`路径下的页面。