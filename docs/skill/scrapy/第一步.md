# Scrapyå¿«é€Ÿä¸Šæ‰‹

Scrapyæ˜¯ä¸€ä¸ªå¼€æºçš„Pythonçˆ¬è™«æ¡†æ¶ã€‚è¿™é‡Œç»™å‡º[å®˜æ–¹æ–‡æ¡£](https://docs.scrapy.org/en/latest/)å’Œä¸€ä¸ª[ä¸­æ–‡æ–‡æ¡£]()ã€‚

> è¿™é‡Œè®°å½•äº†æœ¬äººåœ¨å­¦ä¹ æ–‡æ¡£æ—¶çš„éšè®°ã€‚

## ç¬¬ä¸€æ­¥

### ä»€ä¹ˆæ˜¯Scrapy

Scrapyä¸€ä¸ªè¢«è®¾è®¡ç”¨äº**æŠ“å–å’Œæå–webç«™ç‚¹çš„ç»“æ„åŒ–æ•°æ®**çš„åº”ç”¨æ¡†æ¶ï¼Œä¸è¿‡å®ƒ**ä¹Ÿå¯ä»¥ç”¨APIæå–æ•°æ®**ã€‚

Scrapyçš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯**è¯·æ±‚çš„è°ƒåº¦æ˜¯å¼‚æ­¥çš„**ã€‚

## å®‰è£…

```bash
pip install Scrapy
```

> å€¼å¾—ä¸€æçš„æ˜¯ï¼Œscrapyæ”¯æŒpypy

## æ•™ç¨‹

ä»¥ä¸€ä¸ª[åäººåè¨€ç½‘ç«™](https://quotes.toscrape.com/)ä¸ºä¾‹ï¼Œå®æ“å¦‚ä½•ç”¨Scrapyåˆ¶ä½œä¸€ä¸ªç®€å•çš„çˆ¬è™«ã€‚

åœ¨æ•™ç¨‹ä¸­ï¼Œä½ å°†é¡ºåºè¿›è¡Œå¦‚ä¸‹æ“ä½œï¼š

1. åˆ›å»ºä¸€ä¸ªæ–°çš„Scrapyé¡¹ç›®
2. å†™ä¸€ä¸ª`spider`çˆ¬å–ä¸€ä¸ªç«™ç‚¹å¹¶æå–æ•°æ®
3. ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·å¯¼å‡ºçˆ¬å–åˆ°çš„æ•°æ®
4. é€’å½’åœ°çˆ¬å–è·Ÿéšé“¾æ¥ğŸ”—
5. ä½¿ç”¨çˆ¬è™«å‚æ•°

### æ„å»ºä¸€ä¸ªé¡¹ç›®

åœ¨ä½ å¼€å§‹**çˆ¬**ä¹‹å‰ï¼Œ ä½ éœ€è¦å…ˆåˆ›å»ºçˆ¬è™«é¡¹ç›®ã€‚

```bash
scrapy startproject tutorial
```

è¿™å°†åˆ›å»ºä¸€ä¸ªåä¸º`tutorial`çš„é¡¹ç›®ï¼Œåˆå§‹åŒ–åé¡¹ç›®ä¸­æœ‰å¦‚ä¸‹å†…å®¹:

```
tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py
```

### å†™ä¸€ä¸ªçˆ¬è™«

çˆ¬è™«æ˜¯ä¸€ä¸ªç”±ä½ å®šä¹‰å¹¶ç”±Scrapyç”¨äºçˆ¬å–ç½‘ç«™çš„ç±»ï¼Œå®ƒå¿…é¡»æ˜¯`scrapy.Spider`çš„ä¸€ä¸ªå­ç±»ã€‚

çˆ¬è™«è¦å®šä¹‰å¦‚ä½•å‘é€åˆå§‹è¯·æ±‚ï¼Œå¦‚ä½•è·Ÿè¸ªç½‘é¡µä¸­çš„é“¾æ¥ï¼Œå¦‚ä½•è§£æä¸‹è½½åˆ°çš„ç½‘é¡µå†…å®¹å¹¶ä»ä¸­æå–æ•°æ®ã€‚

ç°åœ¨åœ¨`tutorial/spiders`æ–‡ä»¶å¤¹ä¸‹åˆ›å»ºæ–‡ä»¶`quotes_spider.py`å¹¶å†™å…¥å¦‚ä¸‹å†…å®¹ï¼š

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

As you can seeï½ï¼Œæˆ‘ä»¬çš„çˆ¬è™«ç»§æ‰¿äº†`scrapy.Spider`å¹¶ä¸”å®šä¹‰ä¸€äº›å±æ€§å’Œæ–¹æ³•ï¼š

- `name`ï¼šSpiderçš„å”¯ä¸€æ ‡è¯†ï¼Œåœ¨ä¸€ä¸ªé¡¹ç›®ä¸­å®ƒå¿…é¡»æ˜¯å”¯ä¸€çš„ã€‚
- `start_requests()`ï¼šè¿™ä¸ªæ–¹æ³•å¿…é¡»è¿”å›ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼ˆä½ å¯ä»¥ç›´æ¥è¿”å›ä¸€ä¸ªlistï¼Œæˆ–è€…åƒä»¥ä¸Šä»£ç ä¸€æ ·ï¼Œåªåšä¸€ä¸ªç”Ÿæˆå™¨ï¼‰ï¼Œçˆ¬è™«å°†ä¼šæ ¹æ®è¿™ä¸ªæ–¹æ³•çš„è¿”å›å€¼çˆ¬å–æ•°æ®ã€‚åç»­è¯·æ±‚å°†ä¾æ¬¡ä»è¿™äº›åˆå§‹è¯·æ±‚ä¸­ç”Ÿæˆã€‚
- `parse()`ï¼šç”¨æ¥æ¯ä¸ªè¯·æ±‚è·å–åˆ°çš„å“åº”ã€‚è¿™ä¸ªå‡½æ•°çš„å‚æ•°`response`æ˜¯`TextResponse`çš„ä¸€ä¸ªå®ä¾‹ï¼Œç”¨äºä¿å­˜é¡µé¢å†…å®¹ï¼Œå¹¶å…·æœ‰å¤„ç†é¡µé¢å†…å®¹çš„æ›´å¤šæœ‰ç”¨æ–¹æ³•ã€‚

> `parse()`é€šå¸¸ç”¨æ¥è§£æå“åº”ï¼Œæå–æ•°æ®ï¼Œå¹¶ä¸”ä¹Ÿä¼šç”¨æ¥æ‰¾åˆ°æ–°çš„éœ€è¦çˆ¬å–çš„URLç„¶åæ„å»ºæ–°çš„è¯·æ±‚ã€‚

### å¦‚ä½•è¿è¡Œçˆ¬è™«

åœ¨é¡¹ç›®çš„æ ¹è·¯å¾„ä¸‹ï¼š

```bash
scrapy crawl quotes
```

è¿™ä¸ªå‘½ä»¤çš„ä½¿ç”¨äº†åˆšæ‰å®šä¹‰çš„`name`ä¸º`quotes`çš„çˆ¬è™«ï¼Œç„¶åscrapyå°†ä¼šå‘é€ä¸€äº›è¯·æ±‚åˆ°`quotes.toscrape.com`ï¼Œæ­£å¸¸æƒ…å†µä¸‹ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°è·Ÿä¸€ä¸‹å†…å®¹ç›¸ä¼¼çš„è¾“å‡ºï¼š

```bash
... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
```

ç°åœ¨ä½ åº”è¯¥èƒ½åœ¨é¡¹ç›®ä¸­æ‰¾åˆ°ä¸¤ä¸ªæ–°æ–‡ä»¶:`quotes-1.html`ã€`quotes-2.html`ï¼Œå…¶ä¸­ä¿å­˜åˆ°äº†çˆ¬è™«åˆšåˆšè¯·æ±‚åˆ°çš„ç½‘é¡µå†…å®¹ã€‚

### åˆšåˆšå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

Scrapyè°ƒåº¦çˆ¬è™«çš„`start_requests`æ–¹æ³•è¿”å›çš„`scrapy.Request`å¯¹è±¡ã€‚åœ¨æ”¶åˆ°æ‰€æœ‰å“åº”ä¹‹åï¼ŒScrapyä¼šå®ä¾‹åŒ–`Response`å¯¹è±¡å¹¶è°ƒç”¨ä¸è¯·æ±‚å…³è”çš„å›è°ƒæ–¹æ³•ï¼ˆè¿™é‡Œæ˜¯`parse()`ï¼‰ï¼Œå¹¶å°†å“åº”ä½œä¸ºå‚æ•°ä¼ é€’ã€‚

### ä¸€ä¸ª`start_requests`çš„ç®€å†™

`start_requests`å¹¶ä¸å¿…é¡»æ˜¯ä¸€ä¸ªæ–¹æ³•ï¼Œä»–ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªç±»å±æ€§ã€‚

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
```

### æå–æ•°æ®

æœ€ä½³åœ°å­¦ä¹ ä½¿ç”¨Scrapyæå–æ•°æ®çš„æ–¹å¼æ˜¯é€šè¿‡Scrapy Shellä½¿ç”¨é€‰æ‹©å™¨ï¼Œç°åœ¨è¿è¡Œï¼š

```bash
scrapy shell 'http://quotes.toscrape.com/page/1/'
```

ä½ å°†ä¼šçœ‹åˆ°ï¼š

```bash
[ ... Scrapy log here ... ]
2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>
[s]   item       {}
[s]   request    <GET http://quotes.toscrape.com/page/1/>
[s]   response   <200 http://quotes.toscrape.com/page/1/>
[s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>
[s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
>>>
```

é€šè¿‡è¿™ä¸ªshellï¼Œä½ èƒ½å¤Ÿåœ¨å“åº”å¯¹è±¡ä¸Šä½¿ç”¨CSSé€‰æ‹©å…ƒç´ ï¼š

```bash
>>> response.css('title')
[<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]
```

`response.css('title')`çš„è¿”å›å€¼æ˜¯`SelectorList`å¯¹è±¡ï¼Œå®ƒæ˜¯ä¸€ä¸ªç±»æ•°ç»„ï¼Œç›¸å½“äºä¸€ä¸ªåŒ…è£¹äº†XML/HTMLçš„`Selector`çš„åˆ—è¡¨ï¼Œ`Selector`å…è®¸ç”¨æˆ·è¿›ä¸€æ­¥åœ°æŸ¥è¯¢æˆ–æå–æ•°æ®ã€‚

ä¸ºäº†ä»titleå…ƒç´ ä¸­æå–æ–‡æœ¬ï¼Œä½ å¯ä»¥ï¼š

```bash
>>> response.css('title::text').getall()
['Quotes to Scrape']
```

è¿™é‡Œæœ‰ä¸¤ä¸ªéœ€è¦æ³¨æ„çš„ç‚¹ï¼Œä¸€ä¸ªæ˜¯æˆ‘ä»¬è¿½åŠ äº†`::text`æ¥è¿›è¡ŒCSSæŸ¥è¯¢ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªæƒ³è¦è·å–ç›´æ¥éš¶å±äº`title`æ ‡ç­¾å†…éƒ¨çš„æœ¬æ–‡å…ƒç´ ã€‚å¦‚æœæˆ‘ä»¬ä¸æŒ‡å®š`::text`ï¼Œé‚£æˆ‘ä»¬é¢ä¸´å¦‚ä¸‹ç»“æœï¼š

```bash
>>> response.css('title').getall()
['<title>Quotes to Scrape</title>']
```

å¦ä¸€ä¸ªæ˜¯è°ƒç”¨`.getall()`çš„ç»“æœæ˜¯ä¸€ä¸ªæ•°ç»„ï¼šä¸€ä¸ªselectorå¯èƒ½å°±ä¼šæœ‰å¤šä¸ªè¿”å›å€¼ï¼Œæ­¤æ—¶`.getall()`çš„ç»“æœæ˜¯æ¯ä¸ªselectorçš„å¤šä¸ªè¿”å›å€¼å…±åŒç»„æˆçš„ä¸€ä¸ªæ•°ç»„ï¼ˆä¸€ç»´çš„ï¼Œå¹¶èµ·æ¥äº†ï¼‰ã€‚å¦‚æœä½ åªæƒ³è¦ç¬¬ä¸€ä¸ªç»“æœï¼Œåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œä½ å¯ä»¥è¿™ä¹ˆåšï¼š

```bash
>>> response.css('title::text').get()
'Quotes to Scrape'
```

å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥è¿™æ ·å†™ï¼š

```bash
>>> response.css('title::text')[0].get()
'Quotes to Scrape'
```

ç„¶è€Œï¼Œç›´æ¥åœ¨`SelectorList`ä¸Šä½¿ç”¨`.get()`æ–¹æ³•å¯ä»¥åˆ«é¢å‡ºç°`IndexError`æˆ–è€…`return None`çš„å¯èƒ½ã€‚

å‡ºäº†`getall()`å’Œ`get()`æ–¹æ³•ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨`re()`æ–¹æ³•æ¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥è¯¢ï¼š

```bash
>>> response.css('title::text').re(r'Quotes.*')
['Quotes to Scrape']
>>> response.css('title::text').re(r'Q\w+')
['Quotes']
>>> response.css('title::text').re(r'(\w+) to (\w+)')
['Quotes', 'Scrape']
```

ä¸ºäº†æ–¹ä¾¿æ‰¾åˆ°æ­£ç¡®çš„CSSé€‰æ‹©å™¨ï¼Œä½ å¯ä»¥ä½¿ç”¨`view(response)`æ¥ä½¿ç”¨Webæµè§ˆå™¨æŸ¥çœ‹å“åº”å†…å®¹ï¼Œç„¶åå€ŸåŠ©æµè§ˆå™¨æä¾›çš„å¼€å‘è€…å·¥å…·æ¥è¾…åŠ©æ‰¾åˆ°CSSé€‰æ‹©å™¨ã€‚

### XPathï¼šä¸€ä¸ªç®€å•çš„ä»‹ç»

å‡ºäº†CSSé€‰æ‹©å™¨ï¼ŒScrapy selectorä¹Ÿæ”¯æŒä½¿ç”¨XPathè¡¨è¾¾å¼ï¼š

```bash
>>> response.xpath('//title')
[<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]
>>> response.xpath('//title/text()').get()
'Quotes to Scrape'
```

XPathè¡¨è¾¾å¼ååˆ†å¼ºå¤§ã€‚è€Œä¸”äº‹å®ä¸Šï¼Œåœ¨Scrapyä¸­ä½¿ç”¨çš„CSSé€‰æ‹©å™¨ä¼šè¢«éšå¼åœ°è½¬æ¢ä¸ºXPathç„¶åå†è¿›è¡ŒæŸ¥è¯¢ã€‚

è™½ç„¶å¯èƒ½ä¸åƒ CSS é€‰æ‹©å™¨é‚£æ ·å—æ¬¢è¿ï¼Œä½† XPath è¡¨è¾¾å¼æä¾›äº†æ›´å¤šçš„åŠŸèƒ½ï¼Œå› ä¸ºé™¤äº†å¯¼èˆªç»“æ„ä¹‹å¤–ï¼Œå®ƒè¿˜å¯ä»¥æŸ¥çœ‹å†…å®¹ã€‚å€ŸåŠ©XPathï¼Œä½ å¯ä»¥è¿›è¡Œç±»ä¼¼è¿™ç§æŸ¥è¯¢ï¼šæŸ¥è¯¢åŒ…å«æ–‡æœ¬'Next Page'çš„è¶…é“¾æ¥ã€‚è¿™ä½¿å¾—XPathç‰¹åˆ«é€‚åˆå»æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ï¼Œå¹¶ä¸”å³ä½¿ä½ å·²ç»çŸ¥é“å¦‚ä½•ä½¿ç”¨CSSé€‰æ‹©å™¨äº†ï¼Œæˆ‘ä»¬ä¹Ÿä»ç„¶å»ºè®®ä½ ä½¿ç”¨XPathã€‚æœ¬ç« åªæ˜¯Scrapyçš„ç®€å•ä»‹ç»ï¼Œæˆ‘ä»¬ä¼šåœ¨ä¹‹åçš„ç« èŠ‚è¯¦ç»†ä»‹ç»Scrapyä¸­çš„XPathã€‚

### æå–å¼•æ–‡å’Œä½œè€…

ç°åœ¨ä½ ç®€å•äº†è§£äº†å¦‚ä½•æŸ¥è¯¢å’Œæå–ï¼Œç°åœ¨è®©æˆ‘ä»¬å®Œå–„è¿™ä¸ªçˆ¬è™«ï¼Œè®©ä»–èƒ½å¤Ÿçˆ¬å–ç½‘é¡µä¸­çš„å¼•æ–‡ã€‚

æ¯ä¸ª http://quotes.toscrape.com ä¸­çš„å¼•æ–‡éƒ½æ˜¯è¿™æ ·çš„æ ¼å¼ï¼š

```html
<div class="quote">
    <span class="text">â€œThe world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.â€</span>
    <span>
        by <small class="author">Albert Einstein</small>
        <a href="/author/Albert-Einstein">(about)</a>
    </span>
    <div class="tags">
        Tags:
        <a class="tag" href="/tag/change/page/1/">change</a>
        <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
        <a class="tag" href="/tag/thinking/page/1/">thinking</a>
        <a class="tag" href="/tag/world/page/1/">world</a>
    </div>
</div>
```

ç°åœ¨æˆ‘ä»¬æ‰“å¼€scrapy shellï¼Œç„¶åæ‘¸ç´¢å¦‚ä½•æå–æˆ‘ä»¬æƒ³è¦çš„æ•°æ®ï¼š

```bash
scrapy shell 'http://quotes.toscrape.com'
```

ç„¶åæˆ‘ä»¬è·å–æ‰€æœ‰çš„å¼•æ–‡ï¼š

```bash
>>> response.css("div.quote")
```

ä»¥ä¸ŠæŸ¥è¯¢è¿”å›çš„æ¯ä¸€ä¸ªSelectoréƒ½å…è®¸æˆ‘ä»¬æ›´è¿›ä¸€æ­¥çš„æŸ¥è¯¢ä»–ä»¬çš„å­å…ƒç´ ã€‚è®©æˆ‘ä»¬å°†ç¬¬ä¸€ä¸ªSelectoråˆ†é…ç»™ä¸€ä¸ªå˜é‡ï¼Œç„¶åæˆ‘ä»¬å°±èƒ½ç›´æ¥åœ¨ç¬¬ä¸€ä¸ªSelectorä¸Šåº”ç”¨CSSé€‰æ‹©å™¨ã€‚

```bash
>>> quote = response.css("div.quote")[0]
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»`quote`ä¸­æå–å¼•æ–‡çš„`title`ã€`author`å’Œ`tags`ï¼š

```bash
>>> title = quote.css("span.text::text").get()
>>> title
'â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.â€'
>>> author = quote.css("small.author::text").get()
>>> author
'Albert Einstein'
```

è·å–çš„åˆ°tagsæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`.getall()`æ–¹æ³•è·å–ä»–ä»¬ï¼š

```bash
>>> tags = quote.css("div.tags a.tag::text").getall()
>>> tags
['change', 'deep-thoughts', 'thinking', 'world']
```

ç°åœ¨æˆ‘ä»¬å¼„æ¸…æ¥šäº†å¦‚ä½•æå–æ•°æ®ï¼Œç°åœ¨æˆ‘ä»¬å°±å¯ä»¥è¿­ä»£åœ°è·å–æ‰€æœ‰çš„å¼•æ–‡å…ƒç´ ï¼š

```bash
>>> for quote in response.css("div.quote"):
...     text = quote.css("span.text::text").get()
...     author = quote.css("small.author::text").get()
...     tags = quote.css("div.tags a.tag::text").getall()
...     print(dict(text=text, author=author, tags=tags))
{'tags': ['change', 'deep-thoughts', 'thinking', 'world'], 'author': 'Albert Einstein', 'text': 'â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.â€'}
{'tags': ['abilities', 'choices'], 'author': 'J.K. Rowling', 'text': 'â€œIt is our choices, Harry, that show what we truly are, far more than our abilities.â€'}
    ... a few more of these, omitted for brevity
>>>
```

### åœ¨æˆ‘ä»¬çš„Spiderä¸­æå–æ•°æ®

ç°åœ¨è®©æˆ‘ä»¬å°†ä»¥ä¸Šé€»è¾‘å®ç°åˆ°æˆ‘ä»¬çš„Spiderä¸­ã€‚

é€šå¸¸ä¸€ä¸ªScrapyçš„Spiderä¼šç”Ÿæˆè®¸å¤šå­—å…¸ï¼Œå­—å…¸ä¸­å­˜æ”¾ç€ä»é¡µé¢ä¸­æå–çš„æ•°æ®ã€‚å€ŸåŠ©`yeild`ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹å®ç°Spiderï¼š


```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }
```

è¿è¡Œä»¥ä¸Šä»£ç ï¼Œä½ åº”è¯¥èƒ½å¤Ÿçœ‹åˆ°å¦‚ä¸‹è¾“å‡ºï¼š


```bash
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
{'tags': ['life', 'love'], 'author': 'AndrÃ© Gide', 'text': 'â€œIt is better to be hated for what you are than to be loved for what you are not.â€'}
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': "â€œI have not failed. I've just found 10,000 ways that won't work.â€"}
```

### å­˜å‚¨çˆ¬å–åˆ°çš„æ•°æ®

å­˜å‚¨çˆ¬å–åˆ°çš„æ•°æ®çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨Feedå¯¼å‡ºï¼Œå°±åƒå¦‚ä¸‹å‘½ä»¤ï¼š

```bash
scrapy crawl quotes -o quotes.json
```

ä»¥ä¸Šä»£ç ä¼šç”Ÿæˆä¸€ä¸ª`quotes.json`æ–‡ä»¶ä¿å­˜çˆ¬å–åˆ°çš„æ•°æ®ã€‚

å› ä¸ºä¸€äº›å†å²é—ç•™é—®é¢˜ï¼Œå½“ä½ é‡å¤æ‰§è¡Œè¿™ä¸ªå‘½ä»¤æ—¶ï¼Œæ–‡ä»¶å¹¶ä¸ä¼šè¢«å¤å†™ï¼Œè€Œæ˜¯ä¼šå°†æ–°å†…å®¹è¿½åŠ åˆ°åŸæ¥çš„æ–‡ä»¶æœ€åï¼Œå˜æˆç±»ä¼¼è¿™æ ·çš„`[ç¬¬ä¸€æ¬¡ç»“æœ][ç¬¬äºŒæ¬¡ç»“æœ]`å½¢å¼ã€‚

ä½ ä¹Ÿèƒ½ä½¿ç”¨å…¶ä»–æ ¼å¼å­˜å‚¨æ•°æ®ï¼Œæ¯”å¦‚Json Linesï¼š

```bash
scrapy crawl quotes -o quotes.jl
```

Json Linesé€šå¸¸å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªæµå¼æ•°æ®ï¼Œä»–ä¼šæ›´å®¹æ˜“è¢«æ‰©å±•ï¼Œè€Œä¸ä¼šå‡ºç°ä»¥ä¸ŠJsonæ ¼å¼é‚£æ ·çš„è¿½åŠ é—®é¢˜ã€‚

åœ¨å°é¡¹ç›®ä¸­ï¼Œä»¥ä¸Šçš„æ‰€æœ‰å†…å®¹åº”è¯¥å·²ç»è¶³å¤Ÿäº†ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æƒ³æ‰§è¡Œæ›´å¤æ‚çš„çˆ¬å–ï¼Œä½ å¯èƒ½ä¼šç”¨åˆ°Scrapyçš„ç®¡é“ï¼ˆPipelineï¼‰ã€‚é¡¹ç›®åœ¨åˆå§‹åŒ–æ—¶ï¼Œå·²ç»åœ¨`tutorial/pipelines.py`ä¸­é¢„ç•™å¥½äº†é¡¹ç›®ç®¡é“çš„å ä½æ–‡ä»¶ï¼Œå¦‚æœæ‚¨åªæƒ³å­˜å‚¨æŠ“å–åˆ°çš„é¡¹ç›®ï¼Œåˆ™ä¸éœ€è¦å®ç°ä»»ä½•ç®¡é“ã€‚

### éšåçš„é“¾æ¥

`éšåçš„é“¾æ¥`çš„æ„æ€æ˜¯ï¼Œæˆ‘ä»¬ä¸ä»…è¦ä»Spiderä¸­å£°æ˜çš„ä¸¤ä¸ªç½‘é¡µä¸­çˆ¬å–æ•°æ®ï¼Œè¿˜éœ€è¦ä»ç½‘é¡µä¸­è¶…é“¾æ¥æŒ‡å‘çš„é¡µé¢ä¸­çˆ¬å–æ•°æ®ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å›é¡¾åˆšæ‰çˆ¬å–çš„é¡µé¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ªæŒ‡å‘ä¸‹ä¸€é¡µçš„è¶…é“¾æ¥ï¼š

```html
<ul class="pager">
    <li class="next">
        <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>
    </li>
</ul>
```

æˆ‘ä»¬é¦–å…ˆè·å–åˆ°è¶…é“¾æ¥å…ƒç´ ï¼š

```bash
>>> response.css('li.next a').get()
'<a href="/page/2/">Next <span aria-hidden="true">â†’</span></a>'
```

ç°åœ¨æˆ‘ä»¬è·å–åˆ°çš„äº†å…ƒç´ ï¼Œä½†æ˜¯æˆ‘ä»¬å…¶å®æ˜¯æƒ³è¦è·å–è¿™ä¸ªæ ‡ç­¾çš„`href`å±æ€§å¯¹å§ï¼Œæ­¤æ—¶Scrapyæ”¯æŒä½¿ç”¨æ‰©å±•CSSé€‰æ‹©å™¨ï¼š

```bash
>>> response.css('li.next a::attr(href)').get()
'/page/2/'
```

è¿˜æœ‰ä¸€ç§æ–¹æ³•è·å–å±æ€§ï¼Œå°±æ˜¯hissing`attrib`å±æ€§

```bash
>>> response.css('li.next a').attrib['href']
'/page/2'
```

ç°åœ¨æˆ‘ä»¬ä¿®æ”¹æˆ‘ä»¬çš„çˆ¬è™«ï¼Œé€’å½’åœ°è·Ÿéšé“¾æ¥åˆ°ä¸‹ä¸€é¡µï¼Œå¹¶ä»ä¸­æå–æ•°æ®ï¼š

```bash
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
```

ç°åœ¨ï¼Œæˆ‘ä»¬çš„çˆ¬è™«ä¼šåœ¨çˆ¬å–ä¹‹å‰çš„æ•°æ®ä¹‹åï¼Œå†æ¬¡é€šè¿‡`parse()`å‡½æ•°çˆ¬å–ä¸‹ä¸€é¡µã€‚

è¿™é‡Œæœ‰ä¸€ä¸ªéœ€è¦æ³¨æ„çš„ç‚¹ï¼Œä»¥ä¸Šæˆ‘ä»¬è·å–åˆ°çš„è¶…é“¾æ¥å…¶å®æ˜¯ç›¸å¯¹åœ°å€ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡`urljoin()`å‡½æ•°æ¥æ„é€ ç»å¯¹åœ°å€æ‰èƒ½æ­£å¸¸è®¿é—®ã€‚

çœ‹è¿‡å¦‚ä¸Šä»£ç ï¼Œæˆ‘ä»¬èƒ½å¤Ÿçœ‹å‡ºä¸€ä¸ªScrapyçš„æœºåˆ¶ï¼Œåœ¨`parse()`å‡½æ•°æŠ›å‡ºä¸€ä¸ªRequestå¯¹è±¡æ—¶ï¼ŒScrapyå°†ä¼šè°ƒåº¦è¿™ä¸ªè¯·æ±‚ï¼Œå¹¶åœ¨è¯·æ±‚ç»“æŸæ—¶è°ƒç”¨å›è°ƒæ–¹æ³•ã€‚

### ä¸€ä¸ªæ„å»ºRequestçš„ç®€å†™

ä»¥ä¸Šçš„é€»è¾‘å¯ä»¥è¢«ç²¾ç®€å¦‚ä¸‹ï¼š

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
```

æ²¡é”™ï¼Œ`response.follow()`æ”¯æŒç›¸å¯¹URLè·¯å¾„ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`response.follow()`æ–¹æ³•ä¼šè¿”å›ä¸€ä¸ªRequestå¯¹è±¡ï¼Œæ‰€ä»¥æœ¬è´¨ä¸Šè¿˜æ˜¯`parse()`æŠ›å‡ºäº†ä¸€ä¸ªRequestã€‚

`response.follow()`æ–¹æ³•è¿˜ä¸æ­¢äºæ­¤ï¼Œä»–è¿˜æ”¯æŒè¿›ä¸€æ­¥çš„ç®€å†™ï¼Œå³ä¼ å…¥ä¸€ä¸ªselectorï¼š

```python
for href in response.css('li.next a::attr(href)'):
    yield response.follow(href, callback=self.parse)
```

å†è¿›ä¸€æ­¥ï¼Œä¼ å…¥ä¸€ä¸ª`<a>`æ ‡ç­¾çš„åˆ—è¡¨ä¹Ÿå¯ä»¥ï¼š

```python
for a in response.css('li.next a'):
    yield response.follow(a, callback=self.parse)
```

> éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`response.follow()`ä¸æ”¯æŒä¼ å…¥selectoråˆ—è¡¨ï¼Œå³å†™æ³•`response.follow(response.css('li.next a')[0])`éæ³•çš„ã€‚

### æ–°çš„ä¾‹å­

è¿™é‡Œå†ç»™å‡ºä¸€ä¸ªä»ç½‘ç«™ä¸­çˆ¬å–ä½œè€…ä¿¡æ¯çš„ä¾‹å­ï¼š

```python
import scrapy

class AuthorSpider(scrapy.Spider):
    name = 'author'

    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        # follow links to author pages
        for href in response.css('.author + a::attr(href)'):
            yield response.follow(href, self.parse_author)

        # follow pagination links
        for href in response.css('li.next a::attr(href)'):
            yield response.follow(href, self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).get(default='').strip()

        yield {
            'name': extract_with_css('h3.author-title::text'),
            'birthdate': extract_with_css('.author-born-date::text'),
            'bio': extract_with_css('.author-description::text'),
        }
```

ä»”ç»†çœ‹çœ‹ä¸Šé¢çš„ä»£ç ï¼Œç»“åˆä½ å¯¹ç½‘é¡µçš„ç†è§£ï¼Œä½ å¯èƒ½ä¼šäº§ç”Ÿä¸€ä¸ªæ‹…å¿§ï¼Œä»¥ä¸Šçš„ä»£ç ä¼šä¸ä¼šé‡å¤çˆ¬å–ä½œè€…çš„ä¿¡æ¯å‘¢ï¼Ÿ

ç­”æ¡ˆæ˜¯ä¸å¿…æ‹…å¿ƒï¼ŒScrapyé»˜è®¤ä¼šè¿‡æ»¤æ‰URLé‡å¤çš„è¯·æ±‚ï¼Œé¿å…å¯¹ç‰¹å®šæœåŠ¡çš„å¤§é‡é‡å¤è¯·æ±‚ã€‚å½“ç„¶ï¼Œè¿™ä¸ªä¹Ÿå¯ä»¥é€šè¿‡`DUPEFILTER_CLASS`æ¥é…ç½®æ˜¯å¦è¦è¿‡æ»¤ã€‚

æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›å¸¸è§çš„éœ€æ±‚æ˜¯ä½¿ç”¨å¤šä¸ªé¡µé¢çš„æ•°æ®æ„å»ºä¸€ä¸ªitemï¼Œè¿™å¯èƒ½è¦ç”¨åˆ°å°†æ•°æ®é™„åŠ ç»™å›è°ƒå‡½æ•°çš„æŠ€å·§ã€‚

### ä½¿ç”¨çˆ¬è™«å‚æ•°

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯é€‰çš„`-a`å‚æ•°æ¥ï¼Œåœ¨å¯åŠ¨çˆ¬è™«æ—¶ä¼ å…¥å‚æ•°ï¼š

```bash
scrapy crawl quotes -o quotes-humor.json -a tag=humor
```

è¿™äº›å‚æ•°ä¼šè¢«ä¼ é€’ç»™çˆ¬è™«çš„`__init__`æ–¹æ³•ï¼Œç„¶åå˜æˆçˆ¬è™«çš„ä¸€ä¸ªå±æ€§ã€‚

ç„¶åæˆ‘ä»¬å°±å¯ä»¥è¿™æ ·åˆ©ç”¨ä¼ å…¥å‚æ•°äº†ï¼š

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        url = 'http://quotes.toscrape.com/'
        tag = getattr(self, 'tag', None)
        if tag is not None:
            url = url + 'tag/' + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

åœ¨ä¼ å…¥å‚æ•°ä¸º`tag=humor`æ—¶ï¼Œä½ ä¼šæ³¨æ„åˆ°çˆ¬è™«åªçˆ¬å–äº†`/humor`è·¯å¾„ä¸‹çš„é¡µé¢ã€‚